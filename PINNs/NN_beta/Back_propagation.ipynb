{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "import datetime as dt \n",
    "import autograd.numpy as anp\n",
    "from autograd import grad\n",
    "import random\n",
    "from back_prop_utils import loss, grid_search\n",
    "\n",
    "c = 299792458  # Speed of light in m/s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use definition as in the Extractor class\n",
    "\n",
    "def H_th_function(n, w, length):\n",
    "    '''\n",
    "    Inputs\n",
    "    ------\n",
    "    n: refractive index\n",
    "    w: frequency of light being propagated\n",
    "    length: length of the sample we are modelling\n",
    "    \n",
    "    outputs\n",
    "    -------\n",
    "    returns: output for the transfer function\n",
    "\n",
    "    Method\n",
    "    ------\n",
    "    Equation for transfer function derived from [Input source here]\n",
    "\n",
    "    '''\n",
    "    return (4 * n) / ((n + 1) ** 2) * anp.exp(-1j * (n - 1) * w * length/ c)\n",
    "\n",
    "# Function to round to significant figures\n",
    "def round_to_sig_figs(value, sig_figs):\n",
    "    if value == 0:\n",
    "        return 0\n",
    "    return round(value, sig_figs - int(f\"{value:.1e}\".split('e')[1]) - 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: generate random n,k,d for backprop algo, add noise to this.\n",
    "\n",
    "# frequency range\n",
    "interp = 2**6\n",
    "freqs_THz = np.linspace(0.1, 5, interp)   # 0-5 THz \n",
    "freqs = freqs_THz * 1e12\n",
    "freqs_ang = freqs * 2 * np.pi\n",
    "\n",
    "# Generate random n, k, d\n",
    "n_lims = [2, 4]\n",
    "k_lims = [-0.1, 0]\n",
    "d_lims = [300e-6, 500e-6]\n",
    "\n",
    "# Define significant figures for each parameter\n",
    "n_sig_figs = 6\n",
    "k_sig_figs = 6\n",
    "d_sig_figs = 6\n",
    "\n",
    "# Generate data point\n",
    "n = random.uniform(*n_lims)\n",
    "k = random.uniform(*k_lims)\n",
    "d = random.uniform(*d_lims)\n",
    "\n",
    "# Apply significant figures\n",
    "n = round_to_sig_figs(n, n_sig_figs)\n",
    "k = round_to_sig_figs(k, k_sig_figs)\n",
    "d = round_to_sig_figs(d, d_sig_figs)\n",
    "\n",
    "tf_values = [H_th_function((n+k*1j), f, d) for f in freqs_ang]\n",
    "\n",
    "H_values_clean = np.abs(tf_values)\n",
    "phi_values_clean = np.unwrap(np.angle(tf_values))\n",
    "\n",
    "# add noise or actual signals\n",
    "H_values = H_values_clean + np.random.normal(0, 0.005, size=len(H_values_clean))\n",
    "phi_values = phi_values_clean + np.random.normal(0, 1, size=len(phi_values_clean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(20, 5))\n",
    "\n",
    "axs[0].scatter(freqs_THz, phi_values, s=8, label='noisy phase values')\n",
    "axs[0].scatter(freqs_THz, phi_values_clean, s=8, label='clean phase values')\n",
    "axs[0].set_title('Unwrapped phase of sample values')\n",
    "axs[0].set_xlabel('Frequencies [THz]')\n",
    "axs[0].set_ylabel('Angle [Rad]')\n",
    "\n",
    "axs[1].scatter(freqs_THz, H_values, s=8, label='noisy absolute values')\n",
    "axs[1].scatter(freqs_THz, H_values_clean, s=8, label='clean absolute values')\n",
    "axs[1].set_title('Absolute value of transfer function')\n",
    "axs[1].set_xlabel('Frequencies [THz]')\n",
    "axs[1].set_ylabel('|H|')\n",
    "\n",
    "# Annotate with n, k, d values\n",
    "axs[1].set_ylabel('|H|')\n",
    "\n",
    "# Add a label at the top-left of the entire figure (outside the subplots)\n",
    "fig.text(0.05, 0.99, f'n={n:.3f}, k={k:.3f}, d={1e6*d:.1f}Âµm', \n",
    "         verticalalignment='top', horizontalalignment='left', \n",
    "         bbox=dict(facecolor='white', alpha=0.7, edgecolor='none', boxstyle='round,pad=0.5'))\n",
    "\n",
    "axs[0].legend()\n",
    "axs[1].legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform grid-search for params\n",
    "grid_search(n0=2.0, k0=-0.05, d0=0.0004, H_values=H_values, phi_values=phi_values, freqs_ang=freqs_ang, H_th_function=H_th_function, loss=loss, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert experimental data to PyTorch tensors\n",
    "w_tensor = torch.tensor(freqs_ang, dtype=torch.float32)\n",
    "H_exp_tensor = torch.tensor(H_values, dtype=torch.float32)  # Amplitude\n",
    "phi_exp_tensor = torch.tensor(phi_values, dtype=torch.float32)  # Phase\n",
    "\n",
    "# Define PyTorch Model\n",
    "class TransferFunctionModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Use transformed parameters to enforce constraints\n",
    "        self.n_raw = torch.nn.Parameter(torch.tensor(np.log(3.0), dtype=torch.float32))  # log ensures n > 0\n",
    "        self.k_raw = torch.nn.Parameter(torch.tensor(np.log(0.05), dtype=torch.float32))  # log of positive, negated later\n",
    "        self.d_raw = torch.nn.Parameter(torch.tensor(np.log(400e-6), dtype=torch.float32))  # log ensures d > 0\n",
    "\n",
    "    def forward(self, w):\n",
    "        # Apply transformations to enforce constraints\n",
    "        n = torch.exp(self.n_raw)  # Exp ensures n > 0\n",
    "        k = -torch.exp(self.k_raw)  # -Exp ensures k < 0\n",
    "        d = torch.exp(self.d_raw)  # Exp ensures d > 0\n",
    "        \n",
    "        n_complex = n + 1j * k\n",
    "        H_th = (4 * n_complex) / ((n_complex + 1) ** 2) * torch.exp(-1j * (n_complex - 1) * w * d / c)\n",
    "        return H_th\n",
    "\n",
    "# Initialize model and optimizer\n",
    "model = TransferFunctionModel()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training Loop\n",
    "num_epochs = 1000\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Compute predicted transfer function\n",
    "    H_pred = model(w_tensor)\n",
    "\n",
    "    # Compute amplitude and phase\n",
    "    H_pred_amp = torch.abs(H_pred)\n",
    "    H_pred_phase = torch.angle(H_pred)\n",
    "\n",
    "    # Unwrap the phase using NumPy\n",
    "    H_pred_phase_unwrapped = np.unwrap(H_pred_phase.detach().cpu().numpy())  # Convert to NumPy, unwrap, and convert back to tensor\n",
    "\n",
    "    # Convert the unwrapped phase back to a PyTorch tensor\n",
    "    H_pred_phase_unwrapped = torch.tensor(H_pred_phase_unwrapped, dtype=torch.float32).to(H_pred.device)\n",
    "\n",
    "    # Compute loss\n",
    "    loss_amp = torch.mean((H_pred_amp - H_exp_tensor) ** 2)\n",
    "    loss_phase = torch.mean((H_pred_phase_unwrapped - phi_exp_tensor) ** 2) / torch.var(phi_exp_tensor)  # Normalize phase loss\n",
    "\n",
    "\n",
    "    loss = loss_amp + loss_phase\n",
    "\n",
    "    # Backpropagation\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print progress\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}: Loss = {loss.item()}\")\n",
    "\n",
    "# Optimized parameters\n",
    "print(f\"Optimized n: {torch.exp(model.n_raw).item()}, k: {-torch.exp(model.k_raw).item()}, d: {torch.exp(model.d_raw).item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Experimental area\n",
    "# Convert experimental data to PyTorch tensors\n",
    "w_tensor = torch.tensor(freqs_ang, dtype=torch.float32)\n",
    "H_exp_tensor = torch.tensor(H_values, dtype=torch.float32)  # Amplitude\n",
    "phi_exp_tensor = torch.tensor(phi_values, dtype=torch.float32)  # Phase\n",
    "\n",
    "# Define PyTorch Model\n",
    "class TransferFunctionModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Directly initialize parameters within expected range\n",
    "        self.n = torch.nn.Parameter(torch.tensor(3.0, dtype=torch.float32))  # n > 0\n",
    "        self.k = torch.nn.Parameter(torch.tensor(-0.05, dtype=torch.float32))  # k < 0\n",
    "        self.d = torch.nn.Parameter(torch.tensor(400e-6, dtype=torch.float32))  # d > 0\n",
    "\n",
    "    def forward(self, w):\n",
    "        # Ensure parameters satisfy constraints\n",
    "        n = torch.abs(self.n)  # Ensure n > 0\n",
    "        k = -torch.abs(self.k)  # Ensure k < 0\n",
    "        d = torch.abs(self.d)  # Ensure d > 0\n",
    "\n",
    "        n_complex = n + 1j * k\n",
    "        H_th = (4 * n_complex) / ((n_complex + 1) ** 2) * torch.exp(-1j * (n_complex - 1) * w * d / c)\n",
    "        return H_th\n",
    "\n",
    "# Initialize model and optimizer\n",
    "model = TransferFunctionModel()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.00001)\n",
    "\n",
    "# Training Loop\n",
    "num_epochs = 1000\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Compute predicted transfer function\n",
    "    H_pred = model(w_tensor)\n",
    "\n",
    "    # Compute amplitude and phase\n",
    "    H_pred_amp = torch.abs(H_pred)\n",
    "    H_pred_phase = torch.angle(H_pred)\n",
    "\n",
    "    # Unwrap the phase using NumPy\n",
    "    H_pred_phase_unwrapped = np.unwrap(H_pred_phase.detach().cpu().numpy())  # Convert to NumPy, unwrap, and convert back to tensor\n",
    "\n",
    "    # Convert the unwrapped phase back to a PyTorch tensor\n",
    "    H_pred_phase_unwrapped = torch.tensor(H_pred_phase_unwrapped, dtype=torch.float32).to(H_pred.device)\n",
    "\n",
    "    l = loss(H_exp_tensor, H_pred_amp, phi_exp_tensor, H_pred_phase_unwrapped)\n",
    "\n",
    "    # Backpropagation\n",
    "    l.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print progress\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}: Loss = {loss.item()}\")\n",
    "\n",
    "# Optimized parameters\n",
    "print(f\"Optimized n: {model.n.item()}, k: {model.k.item()}, d: {model.d.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: \n",
    "# Test the outputs by plotting.\n",
    "# Test different initial conditions.\n",
    "# Test single thickness back-prop.\n",
    "# Create training plots."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mphys",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
