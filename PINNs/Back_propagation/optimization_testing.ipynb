{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing back-propagation model optimization parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test for two insights:\n",
    "- How initial conditions (ICs) affect the convergence of the model for a fixed n and k.\n",
    "- How true values affect the performance of ICs.\n",
    "\n",
    "Testing these cases will allow us to understand efficiency and generalizability.\n",
    "\n",
    "In this experiment we will open the parameter space to the following values:\n",
    "- $n \\in [2, 8] $ \n",
    "- $k \\in [-0.1, -10^{-6}] $\n",
    "- $d \\in [300 \\, \\mu\\text{m}, 600 \\, \\mu\\text{m}]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from models import TransferFunctionModel\n",
    "from back_prop_utils import H_th_function, round_to_sig_figs\n",
    "from loss_functions import loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing a range of ICs for a fixed data point \n",
    "We will be exploring two types of IC:\n",
    "- **Homogeneous**: The n and k are both close or both far from the actual value.\n",
    "- **Heterogeneous**: Either n or k is close while the other is far.\n",
    "\n",
    "This should give an understanding of both the impact of ICs on the efficiency of the model as well as how this can be broken down to by parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define 4x4 grid of initial conditions\n",
    "n_IC_values = [2.5, 3.5, 5.5, 7.5]\n",
    "k_IC_values = [-0.09, -0.05, -0.02, -5e-6]\n",
    "\n",
    "# Fixed true values\n",
    "n_true = 3.9\n",
    "k_true = -0.01\n",
    "d_true = 499e-6\n",
    "\n",
    "# Frequency setup\n",
    "interp = 2**6\n",
    "freqs_THz = np.linspace(0.1, 5, interp)\n",
    "freqs = freqs_THz * 1e12\n",
    "freqs_ang = freqs * 2 * np.pi\n",
    "w_tensor = torch.tensor(freqs_ang, dtype=torch.float32)\n",
    "\n",
    "# Generate synthetic experimental data\n",
    "tf_values = H_th_function((n_true + k_true * 1j), w_tensor, d_true)\n",
    "\n",
    "H_values_clean = np.abs(tf_values)\n",
    "phi_values_clean = np.unwrap(np.angle(tf_values))\n",
    "phi_tensor = torch.tensor(phi_values_clean, dtype=torch.float32)\n",
    "\n",
    "\n",
    "# Add noise\n",
    "H_values = H_values_clean + np.random.normal(0, 0.005, size=len(H_values_clean))\n",
    "phi_values = phi_tensor + np.random.normal(0, 1, size=len(phi_values_clean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot actual values and noisy signals \n",
    "fig, axs = plt.subplots(1, 2, figsize=(20, 5))\n",
    "\n",
    "axs[0].scatter(freqs_THz, phi_values, s=8, label='noisy phase values')\n",
    "axs[0].scatter(freqs_THz, phi_values_clean, s=8, label='clean phase values')\n",
    "axs[0].set_title('Unwrapped phase of sample values')\n",
    "axs[0].set_xlabel('Frequencies [THz]')\n",
    "axs[0].set_ylabel('Angle [Rad]')\n",
    "\n",
    "axs[1].scatter(freqs_THz, H_values, s=8, label='noisy absolute values')\n",
    "axs[1].scatter(freqs_THz, H_values_clean, s=8, label='clean absolute values')\n",
    "axs[1].set_title('Absolute value of transfer function')\n",
    "axs[1].set_xlabel('Frequencies [THz]')\n",
    "axs[1].set_ylabel('|H|')\n",
    "\n",
    "# Annotate with n, k, d values\n",
    "axs[1].set_ylabel('|H|')\n",
    "\n",
    "# Add a label at the top-left of the entire figure (outside the subplots)\n",
    "fig.text(0.05, 0.99, f'n={n_true:.3f}, k={k_true:.3f}, d={1e6*d_true:.1f}Âµm', \n",
    "         verticalalignment='top', horizontalalignment='left', \n",
    "         bbox=dict(facecolor='white', alpha=0.7, edgecolor='none', boxstyle='round,pad=0.5'))\n",
    "\n",
    "axs[0].legend()\n",
    "axs[1].legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Check methodology:\n",
    "# - Determine the optimal epoch (check if it is doing this when reporting stats).\n",
    "# - Add method to plot optimal epochs.\n",
    "\n",
    "# Store results\n",
    "results = []\n",
    "\n",
    "# Run optimization for each IC pair\n",
    "for n_IC in n_IC_values:\n",
    "    for k_IC in k_IC_values:\n",
    "        # Initialize model\n",
    "        model = TransferFunctionModel(w_tensor=w_tensor, d=d_true, ICs=[n_IC, k_IC])\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "        # Train the model\n",
    "        model.train_model(loss, H_values, phi_values, optimizer=optimizer, epochs=10000, verbose=False)\n",
    "\n",
    "        # Store results\n",
    "        results.append({\n",
    "            \"IC_n\": n_IC,\n",
    "            \"IC_k\": k_IC,\n",
    "            \"final_n\": model.best_params['n'],\n",
    "            \"final_k\": model.best_params['k'],\n",
    "            \"loss_history\": model.loss_history[-1]  # Final loss\n",
    "        })\n",
    "\n",
    "        print(f\"ICs: (n={n_IC}, k={k_IC}) -> Optimized: n = {model.best_params['n']}, k = {model.best_params['k']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert results to DataFrame\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "# True values\n",
    "true_n = n_true\n",
    "true_k = k_true\n",
    "\n",
    "# Heatmap of final optimized n values\n",
    "plt.figure(figsize=(8, 6))\n",
    "pivot_n = df_results.pivot(index=\"IC_n\", columns=\"IC_k\", values=\"final_n\")\n",
    "ax = sns.heatmap(pivot_n, annot=True, cmap=\"coolwarm\", fmt=\".2f\", cbar_kws={'label': 'Final n'})\n",
    "plt.title(\"Final Optimized n Across Initial Conditions\")\n",
    "plt.xlabel(\"Initial k\")\n",
    "plt.ylabel(\"Initial n\")\n",
    "\n",
    "# Add annotation for true n value\n",
    "ax.text(0.5, -0.2, f\"True n: {true_n:.2f}\", fontsize=12, ha=\"center\", va=\"center\", transform=ax.transAxes)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Heatmap of final optimized k values\n",
    "plt.figure(figsize=(8, 6))\n",
    "pivot_k = df_results.pivot(index=\"IC_n\", columns=\"IC_k\", values=\"final_k\")\n",
    "ax = sns.heatmap(pivot_k, annot=True, cmap=\"coolwarm\", fmt=\".4f\", cbar_kws={'label': 'Final k'})  # Increased precision\n",
    "plt.title(\"Final Optimized k Across Initial Conditions\")\n",
    "plt.xlabel(\"Initial k\")\n",
    "plt.ylabel(\"Initial n\")\n",
    "\n",
    "# Add annotation for true k value\n",
    "ax.text(0.5, -0.2, f\"True k: {true_k:.4f}\", fontsize=12, ha=\"center\", va=\"center\", transform=ax.transAxes)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Heatmap of final loss values\n",
    "plt.figure(figsize=(8, 6))\n",
    "pivot_loss = df_results.pivot(index=\"IC_n\", columns=\"IC_k\", values=\"loss_history\")\n",
    "ax = sns.heatmap(pivot_loss, annot=True, cmap=\"magma\", fmt=\".4g\", cbar_kws={'label': 'Final Loss'})  # Uses 4 significant figures\n",
    "plt.title(\"Final Loss Across Initial Conditions\")\n",
    "plt.xlabel(\"Initial k\")\n",
    "plt.ylabel(\"Initial n\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Impact of Varying True Parameters (n, k) for a Fixed Initial Condition\n",
    "We will explore two types of variations:\n",
    "- **Narrow Range Variations**: We will test values of n and k that are close to their true \n",
    "values but still represent a range of plausible initial conditions. This will help us \n",
    "understand the model's behavior under realistic parameter variations.\n",
    "- **Wide Range Variations**: We will test values of n and k that are significantly different \n",
    "from their true values, representing extreme initial conditions. This will help us understand \n",
    "how robustly the model can be to unusual or outlier-like initial conditions.\n",
    "\n",
    "This experiment aims to investigate how varying the true parameters (n, k) affects the model's \n",
    "performance and behavior for a fixed set of initial conditions, providing insights into the \n",
    "model's parameter sensitivity and robustness."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mphys",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
